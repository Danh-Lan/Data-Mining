{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-12-07T09:00:24.641491Z","iopub.status.busy":"2023-12-07T09:00:24.640646Z","iopub.status.idle":"2023-12-07T09:00:27.083902Z","shell.execute_reply":"2023-12-07T09:00:27.083127Z","shell.execute_reply.started":"2023-12-07T09:00:24.641457Z"},"trusted":true},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import seaborn as sns\n","import random\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n","from sklearn.decomposition import PCA\n","from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n","from sklearn.feature_selection import RFE\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.svm import SVC\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import f1_score\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n","from sklearn.inspection import PartialDependenceDisplay"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-06T07:50:56.187918Z","iopub.status.busy":"2023-12-06T07:50:56.187553Z","iopub.status.idle":"2023-12-06T07:50:56.195400Z","shell.execute_reply":"2023-12-06T07:50:56.194316Z","shell.execute_reply.started":"2023-12-06T07:50:56.187890Z"},"trusted":true},"outputs":[],"source":["# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"markdown","metadata":{},"source":["# 1. Data Preprocessing\n","## 1.1. Data Reading\n","For this challenge we used the small dataset, i.e. TRAIN.CSV. We implemented a function that handles rows having different number of columns. In the end, the obtained dataframe contains a lot of null values.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-06T07:51:01.051872Z","iopub.status.busy":"2023-12-06T07:51:01.051395Z","iopub.status.idle":"2023-12-06T07:51:12.519994Z","shell.execute_reply":"2023-12-06T07:51:12.518267Z","shell.execute_reply.started":"2023-12-06T07:51:01.051826Z"},"trusted":true},"outputs":[],"source":["train_data = \"/kaggle/input/projetdm-data/TRAIN.CSV\"\n","### Loop the data lines\n","with open(train_data, 'r') as temp_f:\n","    # get No of columns in each line\n","    col_count = [ len(l.split(\",\")) for l in temp_f.readlines() ]\n","\n","### Generate column names  (names will be 0, 1, 2, ..., maximum columns - 1)\n","column_names = [i for i in range(0, max(col_count))]\n","df_train = pd.read_csv(train_data,header=None, delimiter=\",\", names=column_names, low_memory=False).astype(str)\n","df_train"]},{"cell_type":"markdown","metadata":{},"source":["## 1.2. Feature extraction"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-06T07:51:12.531250Z","iopub.status.busy":"2023-12-06T07:51:12.530776Z","iopub.status.idle":"2023-12-06T07:51:12.542886Z","shell.execute_reply":"2023-12-06T07:51:12.541820Z","shell.execute_reply.started":"2023-12-06T07:51:12.531211Z"},"trusted":true},"outputs":[],"source":["import re\n","hotkey_actions_to_count = [\"0\",\"1\",\"2\"]\n","hotkeys_to_count = [\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\"]\n","window_size = 3"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-06T07:51:12.545149Z","iopub.status.busy":"2023-12-06T07:51:12.544790Z","iopub.status.idle":"2023-12-06T07:51:12.560810Z","shell.execute_reply":"2023-12-06T07:51:12.559470Z","shell.execute_reply.started":"2023-12-06T07:51:12.545117Z"},"trusted":true},"outputs":[],"source":["# cut_off: take at most the first cut_off second from every game, default 6 minutes\n","# min_game_length: remove too short games, isn't applied if is_Test is True, default 1 minute\n","\n","def extract_features(input_data, is_test = False, cut_off = 6*60, min_game_length = 1*60):\n","    output_data = []\n","    for row in input_data:\n","        count_hotkey_actions = [0] * len(hotkey_actions_to_count)\n","        count_hotkeys = [0] * len(hotkeys_to_count)\n","        new_row = []\n","        new_row.append(row[0])\n","        new_row.append(row[1])\n","        game_length = 0\n","        count_action = 0\n","        count_pattern_3_hotkey = 0\n","        count_pattern_3s = 0\n","        count_pattern_hsh = 0\n","        \n","        for action in row[2:]:\n","            # look for time cell, begin with \"t\"\n","            if isinstance(action, str):\n","                match = re.match(r't(\\d+)', action)\n","                if match:\n","                    time = int(match.group(1))\n","                    game_length = time\n","                    if (game_length > cut_off):\n","                        break\n","\n","            # count 0, 1, 2 at the end of \"hotkey\"\n","            for index, substring in enumerate(hotkey_actions_to_count):\n","                if (action.startswith('hotkey') and action.endswith(substring)):\n","                    count_hotkey_actions[index] += 1\n","\n","            # count hotkey\n","            for index, substring in enumerate(hotkeys_to_count):\n","                if (action.startswith('hotkey') and action[-2] == substring):\n","                    count_hotkeys[index] += 1\n","            \n","            if (action == 's' or action == 'Base'):\n","                count_action += 1\n","            \n","        # there may be more action after the last 't'\n","        game_length += 2\n","            \n","        # frenquency of base, mineral, other action\n","        count_action /= game_length\n","        new_row.append(count_action)\n","        \n","        for i in range(2, len(row) - window_size + 1):\n","            actions_in_window = row[i:i+window_size]\n","            all_start_with_s = all(action.startswith('s') for action in actions_in_window)\n","            if all_start_with_s:\n","                count_pattern_3s += 1\n","            if actions_in_window[0].startswith('hotkey') and actions_in_window[2].startswith('hotkey'):\n","                if actions_in_window[1].startswith('s'):\n","                    count_pattern_hsh += 1\n","                elif actions_in_window[1].startswith('hotkey'):\n","                    count_pattern_3_hotkey += 1\n","        \n","        # frequency of 3-hotkey pattern\n","        count_pattern_3_hotkey /= game_length\n","        new_row.append(count_pattern_3_hotkey)\n","        \n","        #frequency of 3s pattern\n","        count_pattern_3s /= game_length\n","        new_row.append(count_pattern_3s) \n","        \n","        #frequency of hotkey - s - hotkey pattern\n","        count_pattern_hsh /= game_length\n","        new_row.append(count_pattern_hsh) \n","        \n","        \n","        # Calculate the frequency of hotkey\n","        for i in range(len(hotkey_actions_to_count)):\n","            count_hotkey_actions[i] /= game_length\n","\n","        for i in range(len(hotkeys_to_count)):\n","            count_hotkeys[i] /= game_length\n","        \n","        for i in range(len(count_hotkey_actions)):\n","            new_row.append(count_hotkey_actions[i])\n","        for i in range(len(count_hotkeys)):\n","            new_row.append(count_hotkeys[i])\n","            \n","        if (is_test == False):\n","            if (game_length > min_game_length):\n","                output_data.append(new_row)\n","        else:\n","            output_data.append(new_row)\n","    return output_data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-06T07:51:16.535260Z","iopub.status.busy":"2023-12-06T07:51:16.534905Z","iopub.status.idle":"2023-12-06T07:52:26.358909Z","shell.execute_reply":"2023-12-06T07:52:26.357804Z","shell.execute_reply.started":"2023-12-06T07:51:16.535233Z"},"trusted":true},"outputs":[],"source":["converted_train_data = df_train.values\n","output_train_data = extract_features(converted_train_data)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-06T07:52:38.692766Z","iopub.status.busy":"2023-12-06T07:52:38.692056Z","iopub.status.idle":"2023-12-06T07:52:38.726282Z","shell.execute_reply":"2023-12-06T07:52:38.725211Z","shell.execute_reply.started":"2023-12-06T07:52:38.692703Z"},"trusted":true},"outputs":[],"source":["# Convert the table to a DataFrame with headers\n","headers = ['url', 'race', 'action', 'pattern_3_hotkey', 'pattern_3s', 'pattern_hsh', 'hotkey_created', 'hotkey_update', 'hotkey_used',\n","          'key0','key1', 'key2', 'key3', 'key4', 'key5', 'key6',\n","          'key7', 'key8', 'key9']\n","new_df = pd.DataFrame(output_train_data, columns=headers)\n","new_df.head()"]},{"cell_type":"markdown","metadata":{},"source":["# 2. Data Visualization\n","The objective of this section is to get more information about distribution, correlation and eventual relations between features.\n","## 2.1. Histograms\n","We plotted histogram of all features."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-06T07:52:42.598899Z","iopub.status.busy":"2023-12-06T07:52:42.598486Z","iopub.status.idle":"2023-12-06T07:52:46.309855Z","shell.execute_reply":"2023-12-06T07:52:46.308284Z","shell.execute_reply.started":"2023-12-06T07:52:42.598864Z"},"trusted":true},"outputs":[],"source":["# Data visualization\n","# Histograms\n","\n","new_df.hist(bins=15, color='steelblue', edgecolor='black', linewidth=1.0,\n","           xlabelsize=8, ylabelsize=8, grid=False)    \n","plt.tight_layout(rect=(0, 0, 1.2, 1.2))"]},{"cell_type":"markdown","metadata":{},"source":["We got almost Gaussian distribution on action and hotkey_used. In fact, these features highly correlate with the game length, since most actions are hotkey_used and there are more actions in longer games. With a sufficiently large dataset (more than 3000 rows), we can observe an almost normally distributed game length. \n","## 2.2. Bar plot of Base\n","Below is the frequency of three bases. The most frequently used base is the Protoss, followed by Zerg and Terran."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-06T07:52:46.313315Z","iopub.status.busy":"2023-12-06T07:52:46.312088Z","iopub.status.idle":"2023-12-06T07:52:46.538094Z","shell.execute_reply":"2023-12-06T07:52:46.536966Z","shell.execute_reply.started":"2023-12-06T07:52:46.313267Z"},"trusted":true},"outputs":[],"source":["# Bar Plot\n","fig = plt.figure(figsize = (6, 4))\n","title = fig.suptitle(\"Base Frequency\", fontsize=14)\n","fig.subplots_adjust(top=0.85, wspace=0.3)\n","\n","ax = fig.add_subplot(1,1, 1)\n","ax.set_xlabel(\"Base\")\n","ax.set_ylabel(\"Frequency\") \n","w_q = new_df['race'].value_counts()\n","w_q = (list(w_q.index), list(w_q.values))\n","ax.tick_params(axis='both', which='major', labelsize=8.5)\n","bar = ax.bar(w_q[0], w_q[1], color='steelblue', \n","        edgecolor='black', linewidth=1)"]},{"cell_type":"markdown","metadata":{},"source":["## 2.3. Pair-wise Scatter Plots\n","We are interested in identifying patterns and potential insights into the relationship between features. The chosen features for pair-wise scatter plots are action, pattern_3s,hotkey_used, key2, key3, key4 since they represent 3 types of extracted feature: frequency of action, of pattern (3 successive hotkeys) and of specific key. Obviously, we can plot all possible pairs of feature, but it is quite computationally expensive without significant gain of information. We use different colors to represent points of different bases."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-06T07:52:51.864935Z","iopub.status.busy":"2023-12-06T07:52:51.864527Z","iopub.status.idle":"2023-12-06T07:53:15.095813Z","shell.execute_reply":"2023-12-06T07:53:15.094804Z","shell.execute_reply.started":"2023-12-06T07:52:51.864899Z"},"trusted":true},"outputs":[],"source":["# Pair-wise Scatter Plots\n","cols = ['action', 'pattern_3_hotkey','hotkey_used', 'key2', \"key3\", \"key4\", \"race\"]\n","pp = sns.pairplot(new_df[cols], hue = \"race\", height=1.8, aspect=1.8,\n","                  plot_kws=dict(edgecolor=\"k\", linewidth=0.5),\n","                  diag_kind=\"kde\", diag_kws=dict(fill=True))\n","\n","fig = pp.fig \n","fig.subplots_adjust(top=0.93, wspace=0.3)\n","t = fig.suptitle('Key Attributes Pairwise Plots', fontsize=14)"]},{"cell_type":"markdown","metadata":{},"source":["Among three bases, Terran seems to be distinguished more easily. Along the diagonal of the grid, the univariate distribution of Terran is different from the others for most attributes: the peak of Terran's distributions is relatively lower than the one of Zerg and Protoss.\n","Regarding off-diagonal elements, we can see that three bases are mixed together for most cases. We can still isolate some points of Terran, for instance those having high frequency of key_2 but relatively low frequency of action and pattern_3_hotkey. The number of hotkey_used is mostly greater than the operations on one specific key since points are in the top-left corner of the plots. We estimate that there is no significant correlation between variables."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-06T07:55:15.273377Z","iopub.status.busy":"2023-12-06T07:55:15.273000Z","iopub.status.idle":"2023-12-06T07:55:16.963364Z","shell.execute_reply":"2023-12-06T07:55:16.961939Z","shell.execute_reply.started":"2023-12-06T07:55:15.273347Z"},"trusted":true},"outputs":[],"source":["# One-hot encode the specified columns\n","# new_df = new_df.drop(columns = ['race'])\n","new_df = pd.get_dummies(new_df, columns=['race'])"]},{"cell_type":"markdown","metadata":{},"source":["## 2.4. Correlation matrix\n","In order to compute correlation matrix, all variables must be numerical. Since the variable \"race\" is of type Enum, we have two possibilites: one-hot coding or discarding this feature. For now we do one-hot coding and hope that it might be useful for the prediction."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-06T07:55:24.994057Z","iopub.status.busy":"2023-12-06T07:55:24.993648Z","iopub.status.idle":"2023-12-06T07:55:26.676839Z","shell.execute_reply":"2023-12-06T07:55:26.675847Z","shell.execute_reply.started":"2023-12-06T07:55:24.994024Z"},"trusted":true},"outputs":[],"source":["# Correlation matrix\n","correlation_matrix = new_df.iloc[:, 1:].corr(method='spearman')\n","# Create a heatmap using seaborn\n","plt.figure(figsize=(12, 8))\n","sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n","plt.title('Correlation Matrix Heatmap')\n","# Save the figure\n","plt.savefig('correlation_matrix.png') \n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["We can observe three strong correlations. The first one is between pattern_3_hotkey and hotkey_used, which signifies that most hotkeys were used successively. Pattern_3s correlates with race_Zerg and negatively with race_Protoss. This can be explained by players of Zerg rarely do Base or SingleMineral operations.\n","It might be interesting to look at some moderate correlations such as the one between key6 and key7 or between key0 and key9. This totally makes sense when the key is just next to the other.\n","Correlations between races are quite confusing. They are encoded binary attributes with 1 if the race is of the given type and 0 otherwise. When computing correlation matrix, pandas does not take into account this characteristic, and then returns the results which are difficult to interpret."]},{"cell_type":"markdown","metadata":{},"source":["# 3. Training models"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-06T07:55:42.727078Z","iopub.status.busy":"2023-12-06T07:55:42.726650Z","iopub.status.idle":"2023-12-06T07:55:42.733330Z","shell.execute_reply":"2023-12-06T07:55:42.732201Z","shell.execute_reply.started":"2023-12-06T07:55:42.727043Z"},"trusted":true},"outputs":[],"source":["# X, y separation\n","X = new_df.iloc[:, 1:]  # Features (game information)\n","y = new_df.iloc[:, 0]   # Target variable (player's URL)"]},{"cell_type":"markdown","metadata":{},"source":["Now we have a dataset of 20 features and about 3000 observations. Our first objective is to build a simple linear model that can predict players. Since it is difficult to handle 20-dimensional data, we apply dimensionality reduction techniques to facilitate the model's learning process while preserving the maximum amount of information.\n","## 3.1. Dimensionality Reduction\n","### 3.1.1. Principal Component Analysis"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-02T21:18:01.901509Z","iopub.status.busy":"2023-12-02T21:18:01.901121Z","iopub.status.idle":"2023-12-02T21:18:01.942661Z","shell.execute_reply":"2023-12-02T21:18:01.940912Z","shell.execute_reply.started":"2023-12-02T21:18:01.901482Z"},"trusted":true},"outputs":[],"source":["# Apply PCA to calculate explained variance\n","pca = PCA()\n","X_pca = pca.fit_transform(X)\n","\n","# Proportion of variance explained by each principal component\n","explained_variance_ratio = pca.explained_variance_ratio_\n","\n","# Create a DataFrame to display the results\n","df_explained_variance = pd.DataFrame({'Explained Variance Ratio': explained_variance_ratio})\n","df_explained_variance.index = [f'PC{i+1}' for i in range(len(explained_variance_ratio))]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-02T21:18:10.191931Z","iopub.status.busy":"2023-12-02T21:18:10.191454Z","iopub.status.idle":"2023-12-02T21:18:10.620302Z","shell.execute_reply":"2023-12-02T21:18:10.619073Z","shell.execute_reply.started":"2023-12-02T21:18:10.191892Z"},"trusted":true},"outputs":[],"source":["# Plotting the scree plot\n","plt.figure(figsize=(8, 6))\n","plt.plot(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio, marker='o')\n","plt.title('Scree Plot of Explained Variance')\n","plt.xlabel('Principal Components')\n","plt.ylabel('Explained Variance Ratio')\n","plt.xticks(range(1, len(explained_variance_ratio) + 1))\n","plt.grid(True)\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["Around 70% of variance are explained by the two first principle components. We use different colors to display different players on the graph of 2 dimensions: PCA1 and PCA2. We can rapidly realize that these points are not linearly separable: the visual of PCA shows us two big clusters of players."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-02T21:18:17.674158Z","iopub.status.busy":"2023-12-02T21:18:17.673632Z","iopub.status.idle":"2023-12-02T21:18:20.596758Z","shell.execute_reply":"2023-12-02T21:18:20.595533Z","shell.execute_reply.started":"2023-12-02T21:18:17.674115Z"},"trusted":true},"outputs":[],"source":["# Create a DataFrame for plotting\n","df_pca = pd.DataFrame(data=X_pca[:, :2], columns=['PC1', 'PC2'])\n","df_pca['Target'] = y\n","\n","# Visualize the reduced data using a scatterplot\n","plt.figure(figsize=(8, 6))\n","targets = list(set(y))\n","\n","for target in targets:\n","    indices_to_keep = df_pca['Target'] == target\n","    plt.scatter(df_pca.loc[indices_to_keep, 'PC1'],\n","                df_pca.loc[indices_to_keep, 'PC2'],\n","                label=target)\n","\n","plt.xlabel('Principal Component 1')\n","plt.ylabel('Principal Component 2')\n","plt.title('PCA Visualization')\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["### 3.1.2. Linear Discriminant Analysis\n","This is another dimensionality reduction technique that explicitly aims to maximize the separability between classes, which are players in our case. "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-02T21:19:48.395038Z","iopub.status.busy":"2023-12-02T21:19:48.394538Z","iopub.status.idle":"2023-12-02T21:19:48.489018Z","shell.execute_reply":"2023-12-02T21:19:48.487344Z","shell.execute_reply.started":"2023-12-02T21:19:48.395003Z"},"trusted":true},"outputs":[],"source":["# Apply LDA for dimensionality reduction (specify the desired number of components)\n","lda = LinearDiscriminantAnalysis()\n","X_lda = lda.fit_transform(X, y)\n","\n","# Display the reduced dimensions\n","print(f\"Original shape: {X.shape}\")\n","print(f\"Reduced shape: {X_lda.shape}\")"]},{"cell_type":"markdown","metadata":{},"source":["For this dataset, the output of LDA consists of 16 linear discriminant axes that best separate the classes, which can be used for projection or classification. We visualize the data points on the space constructed by two first discriminant axes."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-02T21:29:05.547712Z","iopub.status.busy":"2023-12-02T21:29:05.547246Z","iopub.status.idle":"2023-12-02T21:29:08.341419Z","shell.execute_reply":"2023-12-02T21:29:08.340393Z","shell.execute_reply.started":"2023-12-02T21:29:05.547680Z"},"trusted":true},"outputs":[],"source":["# Create a DataFrame for plotting\n","df_lda = pd.DataFrame(data=X_lda[:, :2], columns=['LDA1', 'LDA2'])\n","df_lda['Target'] = y\n","\n","# Visualize the reduced data using a scatterplot\n","plt.figure(figsize=(8, 6))\n","targets = list(set(y))\n","\n","for target in targets:\n","    indices_to_keep = df_pca['Target'] == target\n","    plt.scatter(df_lda.loc[indices_to_keep, 'LDA1'],\n","                df_lda.loc[indices_to_keep, 'LDA2'],\n","                label=target)\n","\n","plt.xlabel('1st dimension')\n","plt.ylabel('2nd dimension')\n","plt.title('LDA Visualization')\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["We can see that most points are found in the bottom-left corner of the figure and form a huge cluster. Only a few classes can be linearly isolated from the ensemble. From this analysis, we can conclude that a linear model is not a good choice for this dataset. "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-01T11:14:22.201593Z","iopub.status.busy":"2023-12-01T11:14:22.201054Z","iopub.status.idle":"2023-12-01T11:14:22.271675Z","shell.execute_reply":"2023-12-01T11:14:22.270171Z","shell.execute_reply.started":"2023-12-01T11:14:22.201557Z"},"trusted":true},"outputs":[],"source":["# KNN on LDA\n","# Split the data into training and testing sets\n","X_train, X_valid, y_train, y_valid = train_test_split(X_lda, y, test_size=0.2, random_state=42)\n","\n","# Train a classifier (K-Nearest Neighbors in this example)\n","knn = KNeighborsClassifier(n_neighbors=1)\n","knn.fit(X_train, y_train)\n","\n","# Make predictions\n","y_pred = knn.predict(X_valid)"]},{"cell_type":"markdown","metadata":{},"source":["We train a simple K-Nearest Neighbors classifier with different values of k. Surprisingly, the model with best performance is the one with k = 1. It means that the a data point is not affected by its neighbors. Hence, we leave behind the idea of distance-based classifier and go on with tree-based methods.\n","\n","## 3.2. Random Forest\n","### 3.2.1. Hyperparameters tuning"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-06T07:56:03.266515Z","iopub.status.busy":"2023-12-06T07:56:03.265756Z","iopub.status.idle":"2023-12-06T07:56:03.274027Z","shell.execute_reply":"2023-12-06T07:56:03.272956Z","shell.execute_reply.started":"2023-12-06T07:56:03.266478Z"},"trusted":true},"outputs":[],"source":["# Random Forest\n","# Split the data into training and testing sets\n","X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)  # You can adjust the test_size\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-02T21:40:55.248781Z","iopub.status.busy":"2023-12-02T21:40:55.247895Z","iopub.status.idle":"2023-12-02T21:42:47.618640Z","shell.execute_reply":"2023-12-02T21:42:47.617455Z","shell.execute_reply.started":"2023-12-02T21:40:55.248738Z"},"trusted":true},"outputs":[],"source":["\n","from scipy.stats import randint\n","from sklearn.metrics import make_scorer, f1_score\n","\n","# Define the parameter distributions for random search\n","param_dist = {\n","    'n_estimators': randint(50, 150),\n","    'max_depth': [None, 10, 20],\n","    'min_samples_split': randint(2, 10),\n","    'min_samples_leaf': randint(1, 4)\n","}\n","\n","# Create a Random Forest model\n","rf_model = RandomForestClassifier(random_state=42)\n","\n","# Perform random search with 5-fold cross-validation\n","random_search = RandomizedSearchCV(estimator=rf_model, param_distributions=param_dist, cv=5, scoring='f1_macro', n_iter=40, random_state=42, n_jobs=4)\n","random_search.fit(X_train, y_train)\n","\n","# Print the best hyperparameters\n","print(\"Best Hyperparameters:\", random_search.best_params_)\n","\n","# Evaluate the model on the test set\n","best_rf_model = random_search.best_estimator_\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-06T07:56:07.009944Z","iopub.status.busy":"2023-12-06T07:56:07.009539Z","iopub.status.idle":"2023-12-06T07:56:09.123633Z","shell.execute_reply":"2023-12-06T07:56:09.121953Z","shell.execute_reply.started":"2023-12-06T07:56:07.009912Z"},"trusted":true},"outputs":[],"source":["# Optimal Random Forest model\n","optimal_rf_model = RandomForestClassifier(max_depth=20, min_samples_leaf = 1, min_samples_split = 2, n_estimators = 88, random_state = 42)\n","optimal_rf_model.fit(X_train, y_train)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-06T07:56:19.441630Z","iopub.status.busy":"2023-12-06T07:56:19.441230Z","iopub.status.idle":"2023-12-06T07:56:19.509270Z","shell.execute_reply":"2023-12-06T07:56:19.508075Z","shell.execute_reply.started":"2023-12-06T07:56:19.441596Z"},"trusted":true},"outputs":[],"source":["y_pred = optimal_rf_model.predict(X_valid)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-06T07:56:22.027652Z","iopub.status.busy":"2023-12-06T07:56:22.027290Z","iopub.status.idle":"2023-12-06T07:56:22.044621Z","shell.execute_reply":"2023-12-06T07:56:22.043260Z","shell.execute_reply.started":"2023-12-06T07:56:22.027623Z"},"trusted":true},"outputs":[],"source":["# Compute the F1 score\n","f1 = f1_score(y_valid, y_pred, average= 'macro')\n","\n","# Print the F1 score\n","print(f1)"]},{"cell_type":"markdown","metadata":{},"source":["### 3.2.3. Decisions Understanding \n","We want to identify which features have significant impacts on model decisions. We get the feature_importances_ attribute of the Random Forest trained model and sort its values in decreasing order."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-03T16:15:51.837173Z","iopub.status.busy":"2023-12-03T16:15:51.836780Z","iopub.status.idle":"2023-12-03T16:15:52.190837Z","shell.execute_reply":"2023-12-03T16:15:52.189443Z","shell.execute_reply.started":"2023-12-03T16:15:51.837144Z"},"trusted":true},"outputs":[],"source":["# Feature importances\n","importances = optimal_rf_model.feature_importances_\n","std = np.std([tree.feature_importances_ for tree in optimal_rf_model.estimators_], axis=0)\n","\n","feature_names = list(X_train.columns)\n","forest_importances = pd.Series(importances, index=feature_names)\n","sorted_importances = forest_importances.sort_values(ascending = False)\n","print(sorted_importances)\n","\n","fig, ax = plt.subplots()\n","sorted_importances.plot.bar(ax=ax)\n","ax.set_title(\"Feature importances using MDI\")\n","ax.set_ylabel(\"Mean decrease in impurity\")\n","fig.tight_layout()"]},{"cell_type":"markdown","metadata":{},"source":["We can see that there is no significant difference between feature importances: while key features contributes the most to the model reasoning process, the race attributes are much less important, with MDI score at 0.08 and 0.01 respectively. Hence, we could have removed the race attribute from the dataset without significant loss of performance.\n","We are also interested in understanding how decision has been made for a specific observation. We found the library treeinterpreter, a great tool to decrypt decisions made by decision trees. It decomposes a prediction into sum of bias and feature contributions. Please refer to this Github respository for further explanation: https://github.com/andosa/treeinterpreter"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-03T16:09:54.930695Z","iopub.status.busy":"2023-12-03T16:09:54.930256Z","iopub.status.idle":"2023-12-03T16:10:06.923630Z","shell.execute_reply":"2023-12-03T16:10:06.922072Z","shell.execute_reply.started":"2023-12-03T16:09:54.930661Z"},"trusted":true},"outputs":[],"source":["pip install treeinterpreter"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-03T16:10:42.652576Z","iopub.status.busy":"2023-12-03T16:10:42.652195Z","iopub.status.idle":"2023-12-03T16:10:54.370023Z","shell.execute_reply":"2023-12-03T16:10:54.368842Z","shell.execute_reply.started":"2023-12-03T16:10:42.652543Z"},"trusted":true},"outputs":[],"source":["pip install waterfallcharts"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-03T16:24:26.371410Z","iopub.status.busy":"2023-12-03T16:24:26.371057Z","iopub.status.idle":"2023-12-03T16:24:28.723222Z","shell.execute_reply":"2023-12-03T16:24:28.722348Z","shell.execute_reply.started":"2023-12-03T16:24:26.371381Z"},"trusted":true},"outputs":[],"source":["# How decision was made\n","from treeinterpreter import treeinterpreter\n","from waterfall_chart import plot as waterfall\n","\n","prediction, bias, contributions = treeinterpreter.predict(optimal_rf_model, X_valid.values)\n","prediction.shape, bias.shape, contributions.shape\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-03T16:26:37.465067Z","iopub.status.busy":"2023-12-03T16:26:37.464735Z","iopub.status.idle":"2023-12-03T16:26:37.472003Z","shell.execute_reply":"2023-12-03T16:26:37.470741Z","shell.execute_reply.started":"2023-12-03T16:26:37.465040Z"},"trusted":true},"outputs":[],"source":["def create_contrbutions_df(contributions, random_sample, feature_names, class_names):\n","    contribs = contributions[random_sample].tolist()\n","    contribs.insert(0, bias[random_sample])\n","    contribs = np.array(contribs)\n","    contrib_df = pd.DataFrame(data=contribs, index=[\"Base\"] + feature_names, columns=class_names)\n","    prediction = contrib_df[class_names].sum()\n","    contrib_df.loc[\"Prediction\"] = prediction\n","    return contrib_df\n"]},{"cell_type":"markdown","metadata":{},"source":["We implement a function that takes contributions, an observation, list of features and classes as input to return a dataframe of contribution. This contains the prediction result, i.e., the probabilities of an observation belonging to classes. The predicted label is the name of class having the highest value of probability. We can also obtain the contribution of each feature which leads to this prediction."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-03T16:52:16.599360Z","iopub.status.busy":"2023-12-03T16:52:16.598375Z","iopub.status.idle":"2023-12-03T16:52:16.641907Z","shell.execute_reply":"2023-12-03T16:52:16.640790Z","shell.execute_reply.started":"2023-12-03T16:52:16.599305Z"},"trusted":true},"outputs":[],"source":["random_sample = random.randint(1, len(X_valid))\n","class_names = optimal_rf_model.classes_\n","contrib_df = create_contrbutions_df(contributions, random_sample, feature_names, class_names)\n","contrib_df"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-03T17:30:30.377476Z","iopub.status.busy":"2023-12-03T17:30:30.377054Z","iopub.status.idle":"2023-12-03T17:30:30.384365Z","shell.execute_reply":"2023-12-03T17:30:30.382912Z","shell.execute_reply.started":"2023-12-03T17:30:30.377444Z"},"trusted":true},"outputs":[],"source":["def display_prediction_information(index):\n","    target_value = y_valid.iloc[index]\n","    predicted_value = class_names[np.argmax(prediction[index])]\n","    print(\"Selected Sample     : %d\"%index)\n","    print(\"Actual Target Value : %s\"%target_value)\n","    print(\"Predicted Value     : %s\"%predicted_value)\n","    return predicted_value,target_value"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-03T17:30:40.574886Z","iopub.status.busy":"2023-12-03T17:30:40.574433Z","iopub.status.idle":"2023-12-03T17:30:40.581042Z","shell.execute_reply":"2023-12-03T17:30:40.579938Z","shell.execute_reply.started":"2023-12-03T17:30:40.574852Z"},"trusted":true},"outputs":[],"source":["predicted_value,target_value = display_prediction_information(random_sample)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-03T16:59:28.082488Z","iopub.status.busy":"2023-12-03T16:59:28.082116Z","iopub.status.idle":"2023-12-03T16:59:28.090158Z","shell.execute_reply":"2023-12-03T16:59:28.088814Z","shell.execute_reply.started":"2023-12-03T16:59:28.082459Z"},"trusted":true},"outputs":[],"source":["import plotly.graph_objects as go\n","\n","def create_waterfall_chart(contrib_df, class_name, is_target = False):\n","    if is_target:\n","        name = \"Target\"\n","    else:\n","        name = \"Prediction\"\n","    fig = go.Figure(go.Waterfall(\n","        name = name, #orientation = \"h\", \n","        measure = [\"relative\"] * (len(contrib_df)-1) + [\"total\"],\n","        x = contrib_df.index,\n","        y = contrib_df.loc[:, class_name],\n","        connector = {\"mode\":\"between\", \"line\":{\"width\":4, \"color\":\"rgb(0, 0, 0)\", \"dash\":\"solid\"}}\n","    ))\n","\n","    fig.update_layout(title = name + \" : %s\"%class_name)\n","\n","    return fig"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-12-03T17:30:49.457103Z","iopub.status.busy":"2023-12-03T17:30:49.456759Z","iopub.status.idle":"2023-12-03T17:30:49.471420Z","shell.execute_reply":"2023-12-03T17:30:49.469975Z","shell.execute_reply.started":"2023-12-03T17:30:49.457077Z"},"trusted":true},"outputs":[{"ename":"NameError","evalue":"name 'create_waterfall_chart' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32mc:\\Users\\Utilisateur\\Downloads\\ot2-project-dm.ipynb Cell 52\u001b[0m line \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Utilisateur/Downloads/ot2-project-dm.ipynb#X63sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m create_waterfall_chart(contrib_df, predicted_value)\n","\u001b[1;31mNameError\u001b[0m: name 'create_waterfall_chart' is not defined"]}],"source":["create_waterfall_chart(contrib_df, predicted_value)"]},{"cell_type":"markdown","metadata":{},"source":["We use waterfall chart to visualize the feature contributions with respect to the model prediction. The above figure shows details about feature contributions of a correct classification. A high value of prediction (blue bar) signifies that the model is relatively confident in its returned result. A long green bar means a strong contribution of a feature, usually those related to key numbers.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-03T16:46:13.520602Z","iopub.status.busy":"2023-12-03T16:46:13.520265Z","iopub.status.idle":"2023-12-03T16:46:13.535719Z","shell.execute_reply":"2023-12-03T16:46:13.534477Z","shell.execute_reply.started":"2023-12-03T16:46:13.520574Z"},"trusted":true},"outputs":[],"source":["# Find wrong predictions\n","wrong_predictions = []\n","for i in range (1, len(X_valid)):\n","    predicted_value = class_names[np.argmax(prediction[i])]\n","    target = y_valid.iloc[i]\n","    if target != predicted_value:\n","        wrong_predictions.append(i)  "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-03T17:17:32.844714Z","iopub.status.busy":"2023-12-03T17:17:32.843562Z","iopub.status.idle":"2023-12-03T17:17:32.851341Z","shell.execute_reply":"2023-12-03T17:17:32.849789Z","shell.execute_reply.started":"2023-12-03T17:17:32.844679Z"},"trusted":true},"outputs":[],"source":["index_wrong_instance = random.choice(wrong_predictions)\n","display_prediction_information(index_wrong_instance)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-03T16:59:32.185285Z","iopub.status.busy":"2023-12-03T16:59:32.184883Z","iopub.status.idle":"2023-12-03T16:59:32.205144Z","shell.execute_reply":"2023-12-03T16:59:32.203855Z","shell.execute_reply.started":"2023-12-03T16:59:32.185252Z"},"trusted":true},"outputs":[],"source":["contrib_df = create_contrbutions_df(contributions, index_wrong_instance, feature_names, class_names)\n","predicted_value = class_names[np.argmax(prediction[index_wrong_instance])]\n","target_value = y_valid.iloc[index_wrong_instance]\n","create_waterfall_chart(contrib_df, target_value, True)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-03T16:59:38.357992Z","iopub.status.busy":"2023-12-03T16:59:38.357644Z","iopub.status.idle":"2023-12-03T16:59:38.370478Z","shell.execute_reply":"2023-12-03T16:59:38.369595Z","shell.execute_reply.started":"2023-12-03T16:59:38.357964Z"},"trusted":true},"outputs":[],"source":["create_waterfall_chart(contrib_df, predicted_value)"]},{"cell_type":"markdown","metadata":{},"source":["Regarding wrong predictions, we observe that the class of highest probability, i.e. the predicted value, is very close to the second one, i.e. the actual target value. Our hypothesis is that these classes might be difficult instances and that we probably need more information to be able to classify them. It is also interesting to note that sometimes the predicted player's name is quite similar to the actual one, for instance /Arthur and /SadArthur. We could have done better by using the long dataset, with which we can find some useful features to handle non-trivial observations."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-20T14:09:45.204626Z","iopub.status.busy":"2023-11-20T14:09:45.203791Z","iopub.status.idle":"2023-11-20T14:09:45.223702Z","shell.execute_reply":"2023-11-20T14:09:45.222679Z","shell.execute_reply.started":"2023-11-20T14:09:45.204577Z"},"trusted":true},"outputs":[],"source":["# SVM\n","# Standardize the features (optional but recommended for SVM)\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_test_scaled = scaler.transform(X_valid)"]},{"cell_type":"markdown","metadata":{},"source":["## 3.3. Other models\n","With a hope to improve our score on Kaggle, we try to train different classifiers: Support Vector Machine, AdaBoostClassifier and Deep Neural Network. However, Random Forest still gives the best result in terms of F1-score and explicability. This shows the interest of training tree-based models on tabular data: they are less prone to overfitting and very robust to outliers."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-20T14:10:01.665963Z","iopub.status.busy":"2023-11-20T14:10:01.665215Z","iopub.status.idle":"2023-11-20T14:10:21.832119Z","shell.execute_reply":"2023-11-20T14:10:21.830536Z","shell.execute_reply.started":"2023-11-20T14:10:01.665920Z"},"trusted":true},"outputs":[],"source":["from scipy.stats import loguniform\n","# Define the SVM model\n","svm_model = SVC()\n","\n","# Define the parameter distributions for RandomizedSearchCV\n","param_dist = {\n","    'C': loguniform(1e-3, 1e3),        # Regularization parameter\n","    'kernel': ['linear', 'rbf', 'poly'],  # Kernel type\n","    'gamma': ['scale', 'auto', 'scale'],  # Kernel coefficient for 'rbf' kernel\n","    'degree': randint(2, 5),            # Degree for 'poly' kernel\n","    'shrinking': [True, False],         # Whether to use the shrinking heuristic\n","    'class_weight': [None, 'balanced'], # Class weights\n","}\n","\n","# Create RandomizedSearchCV object\n","rand_search = RandomizedSearchCV(estimator=svm_model, param_distributions=param_dist, cv=5, scoring='f1_macro', n_jobs=4)\n","\n","# Fit the model to the training data\n","rand_search.fit(X_train_scaled, y_train)\n","\n","# Print the best hyperparameters\n","print(\"Best Hyperparameters:\", rand_search.best_params_)\n","# Evaluate the model on the test set\n","best_svm_model = rand_search.best_estimator_\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-20T14:11:16.501279Z","iopub.status.busy":"2023-11-20T14:11:16.500180Z","iopub.status.idle":"2023-11-20T14:11:17.041771Z","shell.execute_reply":"2023-11-20T14:11:17.040414Z","shell.execute_reply.started":"2023-11-20T14:11:16.501232Z"},"trusted":true},"outputs":[],"source":["# Make predictions on the test set\n","y_pred_svm = best_svm_model.predict(X_test_scaled)\n","# Compute the F1 score\n","f1 = f1_score(y_valid, y_pred_svm, average= 'macro')\n","\n","# Print the F1 score\n","print(f1)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-20T14:39:32.664578Z","iopub.status.busy":"2023-11-20T14:39:32.664121Z","iopub.status.idle":"2023-11-20T14:39:34.637196Z","shell.execute_reply":"2023-11-20T14:39:34.635959Z","shell.execute_reply.started":"2023-11-20T14:39:32.664541Z"},"trusted":true},"outputs":[],"source":["# Create AdaBoostClassifier with DecisionTree base estimator\n","base_estimator = DecisionTreeClassifier(max_depth=50)\n","ada_model = AdaBoostClassifier(estimator=base_estimator, random_state=42)\n","\n","# Define hyperparameter space for RandomizedSearchCV\n","param_dist = {\n","    'n_estimators': randint(50, 200),  # Number of estimators\n","    'learning_rate': [0.01, 0.1, 0.5, 1.0],  # Learning rate\n","}\n","\n","# Create RandomizedSearchCV object\n","random_search = RandomizedSearchCV(estimator=ada_model, param_distributions=param_dist, n_iter=10, cv=5, scoring='f1_macro', random_state=42, n_jobs=4)\n","\n","# Fit the RandomizedSearchCV to the training data\n","random_search.fit(X_train, y_train)\n","\n","# Get the best model from RandomizedSearchCV\n","best_ada_model = random_search.best_estimator_\n","\n","# Make predictions on the test set using the best model\n","y_pred = best_ada_model.predict(X_valid)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-20T14:39:40.201258Z","iopub.status.busy":"2023-11-20T14:39:40.200820Z","iopub.status.idle":"2023-11-20T14:39:40.224645Z","shell.execute_reply":"2023-11-20T14:39:40.222971Z","shell.execute_reply.started":"2023-11-20T14:39:40.201226Z"},"trusted":true},"outputs":[],"source":["# Compute the F1 score\n","f1 = f1_score(y_valid, y_pred, average= 'macro')\n","\n","# Print the F1 score\n","print(f1)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-20T16:24:21.570289Z","iopub.status.busy":"2023-11-20T16:24:21.569842Z","iopub.status.idle":"2023-11-20T16:24:21.589823Z","shell.execute_reply":"2023-11-20T16:24:21.588369Z","shell.execute_reply.started":"2023-11-20T16:24:21.570254Z"},"trusted":true},"outputs":[],"source":["print(X)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-03T17:41:18.062094Z","iopub.status.busy":"2023-12-03T17:41:18.061674Z","iopub.status.idle":"2023-12-03T17:41:27.908210Z","shell.execute_reply":"2023-12-03T17:41:27.907011Z","shell.execute_reply.started":"2023-12-03T17:41:18.062062Z"},"trusted":true},"outputs":[],"source":["# Deep Neural Networks\n","import tensorflow as tf\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder, StandardScaler\n","import pandas as pd\n","\n","# Encoding the target attribute for multi-class classification\n","le = LabelEncoder().fit(y)\n","y_enc = le.transform(y)\n","\n","# Split the data into training and testing sets\n","X_train, X_valid, y_train, y_valid = train_test_split(X, y_enc, test_size=0.2, random_state=42)\n","\n","# Normalize/Scale the features\n","scaler = StandardScaler()\n","X_train = scaler.fit_transform(X_train)\n","X_valid = scaler.transform(X_valid)\n","\n","# Build the neural network model using Keras\n","model = tf.keras.Sequential([\n","    tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],), kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n","    tf.keras.layers.Dense(32, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n","    tf.keras.layers.Dense(16, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n","    tf.keras.layers.Dense(len(le.classes_), activation='softmax')  # Multi-class, so using 'softmax'\n","])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-03T17:41:31.213693Z","iopub.status.busy":"2023-12-03T17:41:31.213287Z","iopub.status.idle":"2023-12-03T17:41:42.390294Z","shell.execute_reply":"2023-12-03T17:41:42.389264Z","shell.execute_reply.started":"2023-12-03T17:41:31.213661Z"},"trusted":true},"outputs":[],"source":["# Compile the model\n","model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=[\"accuracy\"])\n","\n","# Train the model\n","model.fit(X_train, y_train, epochs=64, batch_size=32)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-03T17:41:45.635814Z","iopub.status.busy":"2023-12-03T17:41:45.635429Z","iopub.status.idle":"2023-12-03T17:41:45.884169Z","shell.execute_reply":"2023-12-03T17:41:45.882999Z","shell.execute_reply.started":"2023-12-03T17:41:45.635786Z"},"trusted":true},"outputs":[],"source":["# Evaluate the model on test data\n","loss, accuracy = model.evaluate(X_valid, y_valid)\n","print(f\"Test accuracy: {accuracy:.4f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-03T17:41:51.233482Z","iopub.status.busy":"2023-12-03T17:41:51.233126Z","iopub.status.idle":"2023-12-03T17:41:51.406323Z","shell.execute_reply":"2023-12-03T17:41:51.404661Z","shell.execute_reply.started":"2023-12-03T17:41:51.233453Z"},"trusted":true},"outputs":[],"source":["# Get predictions on the test set\n","y_pred_probs = model.predict(X_valid)\n","\n","# Convert probabilities to class labels\n","y_pred_indices = np.argmax(y_pred_probs, axis=1)\n","\n","# Inverse transform the predicted indices to original labels\n","decoded_labels = le.inverse_transform(y_pred_indices)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-03T17:41:54.793485Z","iopub.status.busy":"2023-12-03T17:41:54.792155Z","iopub.status.idle":"2023-12-03T17:41:54.802297Z","shell.execute_reply":"2023-12-03T17:41:54.800589Z","shell.execute_reply.started":"2023-12-03T17:41:54.793425Z"},"trusted":true},"outputs":[],"source":["# 'decoded_labels' now contains the original labels predicted by the model\n","print(decoded_labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-03T17:42:24.619606Z","iopub.status.busy":"2023-12-03T17:42:24.619187Z","iopub.status.idle":"2023-12-03T17:42:25.464701Z","shell.execute_reply":"2023-12-03T17:42:25.463264Z","shell.execute_reply.started":"2023-12-03T17:42:24.619583Z"},"trusted":true},"outputs":[],"source":["# Test data preparation\n","\n","test_data = \"/kaggle/input/projetdm-data/TEST.CSV\"\n","### Loop the data lines\n","with open(test_data, 'r') as temp_f:\n","    # get No of columns in each line\n","    col_count = [ len(l.split(\",\")) for l in temp_f.readlines() ]\n","\n","### Generate column names  (names will be 0, 1, 2, ..., maximum columns - 1)\n","column_names = [i for i in range(0, max(col_count))]\n","df_test = pd.read_csv(test_data,header=None, delimiter=\",\", names=column_names, low_memory=False).astype(str)\n","df_test.insert(0, '0', 0)\n","df_test.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-03T17:42:29.323745Z","iopub.status.busy":"2023-12-03T17:42:29.323302Z","iopub.status.idle":"2023-12-03T17:42:34.100380Z","shell.execute_reply":"2023-12-03T17:42:34.098373Z","shell.execute_reply.started":"2023-12-03T17:42:29.323711Z"},"trusted":true},"outputs":[],"source":["converted_test_data = df_test.values\n","output_test_data = extract_features(converted_test_data, is_test = True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-03T17:42:36.544330Z","iopub.status.busy":"2023-12-03T17:42:36.543658Z","iopub.status.idle":"2023-12-03T17:42:36.666821Z","shell.execute_reply":"2023-12-03T17:42:36.664945Z","shell.execute_reply.started":"2023-12-03T17:42:36.544307Z"},"trusted":true},"outputs":[],"source":["new_test_df = pd.DataFrame(output_test_data, columns=headers)\n","# One-hot encode the specified columns\n","new_test_df = pd.get_dummies(new_test_df, columns=['race'])\n","X_test = new_test_df.iloc[:, 1:]  # Features (game information)\n","predictions = optimal_rf_model.predict(X_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-03T17:42:38.417828Z","iopub.status.busy":"2023-12-03T17:42:38.417474Z","iopub.status.idle":"2023-12-03T17:42:38.425015Z","shell.execute_reply":"2023-12-03T17:42:38.423410Z","shell.execute_reply.started":"2023-12-03T17:42:38.417802Z"},"trusted":true},"outputs":[],"source":["len(new_test_df)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-03T17:43:51.648812Z","iopub.status.busy":"2023-12-03T17:43:51.648446Z","iopub.status.idle":"2023-12-03T17:43:51.658821Z","shell.execute_reply":"2023-12-03T17:43:51.657435Z","shell.execute_reply.started":"2023-12-03T17:43:51.648787Z"},"trusted":true},"outputs":[],"source":["# Convert predictions to a DataFrame\n","submission_df = pd.DataFrame({'prediction': predictions})\n","submission_df['RowId'] = submission_df.index + 1 # Adding row IDs starting from 0\n","\n","# Reorder the columns with 'RowID' as the first column\n","submission_df = submission_df[['RowId', 'prediction']]\n","\n","\n","# Save the DataFrame to a CSV file named 'submissions.csv'\n","submission_df.to_csv('submissions.csv', index=False)  "]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":6985693,"sourceId":63689,"sourceType":"competition"},{"datasetId":3963175,"sourceId":6899377,"sourceType":"datasetVersion"}],"dockerImageVersionId":30558,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"}},"nbformat":4,"nbformat_minor":4}
